{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run any of this you need to have the deepmoji files downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inproceedings{felbo2017,\n",
    "  title={Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm},\n",
    "  author={Felbo, Bjarke and Mislove, Alan and S{\\o}gaard, Anders and Rahwan, Iyad and Lehmann, Sune},\n",
    "  booktitle={Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n",
    "  year={2017}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" Use DeepMoji to score texts for emoji distribution.\n",
    "\n",
    "The resulting emoji ids (0-63) correspond to the mapping\n",
    "in emoji_overview.png file at the root of the DeepMoji repo.\n",
    "\n",
    "Writes the result to a csv file.\n",
    "\"\"\"\n",
    "from __future__ import print_function, division\n",
    "import example_helper\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deepmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from deepmoji.model_def import deepmoji_emojis\n",
    "from deepmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "\n",
    "OUTPUT_PATH = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id_str</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/29/18 12:32</td>\n",
       "      <td>1.068120e+18</td>\n",
       "      <td>False</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Billions of Dollars are pouring into the coffe...</td>\n",
       "      <td>-0.1154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/29/18 12:16</td>\n",
       "      <td>1.068120e+18</td>\n",
       "      <td>False</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>When will this illegal Joseph McCarthy style W...</td>\n",
       "      <td>-0.3793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/29/18 11:54</td>\n",
       "      <td>1.068110e+18</td>\n",
       "      <td>False</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Did you ever see an investigation more in sear...</td>\n",
       "      <td>-0.5769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/29/18 11:37</td>\n",
       "      <td>1.068110e+18</td>\n",
       "      <td>False</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>General Motors is very counter to what other a...</td>\n",
       "      <td>0.0385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11/29/18 4:39</td>\n",
       "      <td>1.068000e+18</td>\n",
       "      <td>False</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>So much happening with the now discredited Wit...</td>\n",
       "      <td>-0.3636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       created_at        id_str is_retweet              source  \\\n",
       "0  11/29/18 12:32  1.068120e+18      False  Twitter for iPhone   \n",
       "1  11/29/18 12:16  1.068120e+18      False  Twitter for iPhone   \n",
       "2  11/29/18 11:54  1.068110e+18      False  Twitter for iPhone   \n",
       "3  11/29/18 11:37  1.068110e+18      False  Twitter for iPhone   \n",
       "4   11/29/18 4:39  1.068000e+18      False  Twitter for iPhone   \n",
       "\n",
       "                                                text  sent_score  \n",
       "0  Billions of Dollars are pouring into the coffe...     -0.1154  \n",
       "1  When will this illegal Joseph McCarthy style W...     -0.3793  \n",
       "2  Did you ever see an investigation more in sear...     -0.5769  \n",
       "3  General Motors is very counter to what other a...      0.0385  \n",
       "4  So much happening with the now discredited Wit...     -0.3636  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../../TrumpNov29WithSScore.csv')\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "dates = []\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        if row['is_retweet']:\n",
    "            continue\n",
    "        cleanwords = []\n",
    "        text = row['text'].split()\n",
    "        for i in range(len(text)):\n",
    "            if text[i].find(\"http\") >= 0:\n",
    "                continue\n",
    "            try:\n",
    "                word = unicode(text[i])\n",
    "                cleanwords.append(text[i])\n",
    "            except Exception as e:\n",
    "                word = text[i].decode('utf-8')\n",
    "                word = word.encode('ascii', 'ignore')\n",
    "                cleanwords.append(word)\n",
    "                \n",
    "        if len(cleanwords) > 1:\n",
    "            sentences.append(unicode(' '.join(cleanwords)))\n",
    "            dates.append(row['created_at'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "for i in sentences:\n",
    "    if len(i.split()) == 1:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34856 34856\n"
     ]
    }
   ],
   "source": [
    "print(len(dates), len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing using dictionary from /Users/JackBelluche/Desktop/Code/BigData/finalProject/DM/DeepMoji/model/vocabulary.json\n",
      "Loading model from /Users/JackBelluche/Desktop/Code/BigData/finalProject/DM/DeepMoji/model/deepmoji_weights.hdf5.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 256)      12800000    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 30, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, 30, 1024)     3149824     activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       (None, 30, 1024)     6295552     bi_lstm_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 30, 2304)     0           bi_lstm_1[0][0]                  \n",
      "                                                                 bi_lstm_0[0][0]                  \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attlayer (AttentionWeightedAver (None, 2304)         2304        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 64)           147520      attlayer[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 22,395,200\n",
      "Trainable params: 22,395,200\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Running predictions.\n",
      "Writing results to test.csv\n"
     ]
    }
   ],
   "source": [
    "# TEST_SENTENCES = [u'I love mom\\'s cooking',\n",
    "#                   u'I love how you never reply back..',\n",
    "#                   u'I love cruising with my homies',\n",
    "#                   u'I love messing with yo mind!!',\n",
    "#                   u'I love you and now you\\'re just gone..',\n",
    "#                   u'This',\n",
    "#                   u'This is the shit']\n",
    "\n",
    "TEST_SENTENCES = sentences\n",
    "\n",
    "\n",
    "\n",
    "def top_elements(array, k):\n",
    "    ind = np.argpartition(array, -k)[-k:]\n",
    "    return ind[np.argsort(array[ind])][::-1]\n",
    "\n",
    "\n",
    "maxlen = 30\n",
    "batch_size = 32\n",
    "\n",
    "print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "st = SentenceTokenizer(vocabulary, maxlen)\n",
    "tokenized, _, _ = st.tokenize_sentences(TEST_SENTENCES)\n",
    "\n",
    "print('Loading model from {}.'.format(PRETRAINED_PATH))\n",
    "model = deepmoji_emojis(maxlen, PRETRAINED_PATH)\n",
    "model.summary()\n",
    "\n",
    "print('Running predictions.')\n",
    "prob = model.predict(tokenized)\n",
    "\n",
    "# Find top emojis for each sentence. Emoji ids (0-63)\n",
    "# correspond to the mapping in emoji_overview.png\n",
    "# at the root of the DeepMoji repo.\n",
    "print('Writing results to {}'.format(OUTPUT_PATH))\n",
    "scores = []\n",
    "for i, t in enumerate(TEST_SENTENCES):\n",
    "    t_tokens = tokenized[i]\n",
    "    t_score = [t]\n",
    "    t_score.append(dates[i])\n",
    "    t_prob = prob[i]\n",
    "    ind_top = top_elements(t_prob, 5)\n",
    "    t_score.append(sum(t_prob[ind_top]))\n",
    "    t_score.extend(ind_top)\n",
    "    t_score.extend([t_prob[ind] for ind in ind_top])\n",
    "    scores.append(t_score)\n",
    "\n",
    "with open(OUTPUT_PATH, 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', lineterminator='\\n')\n",
    "    writer.writerow(['created_at','Text', 'Top5%',\n",
    "                     'Emoji_1', 'Emoji_2', 'Emoji_3', 'Emoji_4', 'Emoji_5',\n",
    "                     'Pct_1', 'Pct_2', 'Pct_3', 'Pct_4', 'Pct_5'])\n",
    "    for i, row in enumerate(scores):\n",
    "        try:\n",
    "            writer.writerow(row)\n",
    "        except Exception as e:\n",
    "            print(\"Exception at row {}!\".format(i))\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>Text</th>\n",
       "      <th>Top5%</th>\n",
       "      <th>Emoji_1</th>\n",
       "      <th>Emoji_2</th>\n",
       "      <th>Emoji_3</th>\n",
       "      <th>Emoji_4</th>\n",
       "      <th>Emoji_5</th>\n",
       "      <th>Pct_1</th>\n",
       "      <th>Pct_2</th>\n",
       "      <th>Pct_3</th>\n",
       "      <th>Pct_4</th>\n",
       "      <th>Pct_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Billions of Dollars are pouring into the coffe...</td>\n",
       "      <td>11/29/18 12:32</td>\n",
       "      <td>0.290665</td>\n",
       "      <td>32</td>\n",
       "      <td>55</td>\n",
       "      <td>33</td>\n",
       "      <td>62</td>\n",
       "      <td>25</td>\n",
       "      <td>0.100459</td>\n",
       "      <td>0.071994</td>\n",
       "      <td>0.042611</td>\n",
       "      <td>0.038831</td>\n",
       "      <td>0.036771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When will this illegal Joseph McCarthy style W...</td>\n",
       "      <td>11/29/18 12:16</td>\n",
       "      <td>0.425414</td>\n",
       "      <td>32</td>\n",
       "      <td>46</td>\n",
       "      <td>55</td>\n",
       "      <td>34</td>\n",
       "      <td>27</td>\n",
       "      <td>0.105558</td>\n",
       "      <td>0.103549</td>\n",
       "      <td>0.080169</td>\n",
       "      <td>0.077162</td>\n",
       "      <td>0.058977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did you ever see an investigation more in sear...</td>\n",
       "      <td>11/29/18 11:54</td>\n",
       "      <td>0.427425</td>\n",
       "      <td>32</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>41</td>\n",
       "      <td>25</td>\n",
       "      <td>0.157205</td>\n",
       "      <td>0.103903</td>\n",
       "      <td>0.064565</td>\n",
       "      <td>0.056425</td>\n",
       "      <td>0.045328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>General Motors is very counter to what other a...</td>\n",
       "      <td>11/29/18 11:37</td>\n",
       "      <td>0.403117</td>\n",
       "      <td>32</td>\n",
       "      <td>55</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>37</td>\n",
       "      <td>0.129634</td>\n",
       "      <td>0.109936</td>\n",
       "      <td>0.073316</td>\n",
       "      <td>0.051891</td>\n",
       "      <td>0.038339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So much happening with the now discredited Wit...</td>\n",
       "      <td>11/29/18 4:39</td>\n",
       "      <td>0.292888</td>\n",
       "      <td>12</td>\n",
       "      <td>62</td>\n",
       "      <td>32</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>0.065822</td>\n",
       "      <td>0.065414</td>\n",
       "      <td>0.056404</td>\n",
       "      <td>0.055462</td>\n",
       "      <td>0.049786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          created_at            Text  \\\n",
       "0  Billions of Dollars are pouring into the coffe...  11/29/18 12:32   \n",
       "1  When will this illegal Joseph McCarthy style W...  11/29/18 12:16   \n",
       "2  Did you ever see an investigation more in sear...  11/29/18 11:54   \n",
       "3  General Motors is very counter to what other a...  11/29/18 11:37   \n",
       "4  So much happening with the now discredited Wit...   11/29/18 4:39   \n",
       "\n",
       "      Top5%  Emoji_1  Emoji_2  Emoji_3  Emoji_4  Emoji_5     Pct_1     Pct_2  \\\n",
       "0  0.290665       32       55       33       62       25  0.100459  0.071994   \n",
       "1  0.425414       32       46       55       34       27  0.105558  0.103549   \n",
       "2  0.427425       32       55       12       41       25  0.157205  0.103903   \n",
       "3  0.403117       32       55       19       25       37  0.129634  0.109936   \n",
       "4  0.292888       12       62       32       52       43  0.065822  0.065414   \n",
       "\n",
       "      Pct_3     Pct_4     Pct_5  \n",
       "0  0.042611  0.038831  0.036771  \n",
       "1  0.080169  0.077162  0.058977  \n",
       "2  0.064565  0.056425  0.045328  \n",
       "3  0.073316  0.051891  0.038339  \n",
       "4  0.056404  0.055462  0.049786  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
